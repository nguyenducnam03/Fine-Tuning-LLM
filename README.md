# PEFT Experiments ðŸš€
Experiments with Parameter-Efficient Fine-Tuning techniques for large language models (LLMs), including **LoRA**, **P-Tuning**, and **Prompt Tuning**.

## ðŸ“Œ Overview
This repository contains hands-on implementations and experiments applying various PEFT techniques on transformer models (e.g., BERT, T5, GPT-2), using Hugging Face and PyTorch.

The goal is to explore the effectiveness of lightweight fine-tuning methods that reduce computational cost while maintaining model performance.

## ðŸ§­ Next Steps
- Apply LoRA to **Mistral-7B** and **LLaMA** models.
- Apply QLoRA also.

